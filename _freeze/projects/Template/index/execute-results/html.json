{
  "hash": "1a89f5a84af659160f81295bd1f63df8",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndate: '2024-03-08'\ntitle: \"Example Paper\"\ncategories: [Paper, Python, Dataviz]\nimage: \"/assets/Iris_pca.png\"\ntitle-block-banner: false\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-tools: true\n    link-external-icon: true\n    link-external-newwindow: true\n---\n\n::: {#685f9ed5 .cell execution_count=1}\n``` {.python .cell-code}\nimport io\nfrom sklearn import get_config\nfrom config import save_system\nfrom encryption import RandomState\nimport tomli\nfrom contextlib import redirect_stdout\n\n# remove stupid advert\nf = io.StringIO()\nwith redirect_stdout(f):\n  import ydata_profiling as yd\n\nfrom pycaret.datasets import get_data\nfrom pycaret.classification.functional import setup, get_config, compare_models, create_model, pull, finalize_model, save_model, plot_model\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams['font.family'] = 'DeJavu Serif'\nplt.rcParams['font.serif'] = ['Times New Roman']\nfrom yellowbrick.features import rank2d\nimport yellowbrick as yb\nimport dtreeviz\n```\n:::\n\n\n::: {#bb3d3863 .cell execution_count=2}\n``` {.python .cell-code}\nsave_system()\n\nwith open(\"../../pyproject.toml\", mode=\"rb\") as fp:\n  config = tomli.load(fp)\n\nPROJECT_NAME = config[\"tool\"][\"poetry\"][\"name\"]\nRANDOM_STATE = RandomState(PROJECT_NAME).state\n```\n:::\n\n\n# Classifying Irises by Field Measurements\n\n*Classification is a fundamental machine learning technique used to predict the group membership of data instances based on input features. The Iris dataset, a well-known benchmark in machine learning, presents a classification challenge involving three species of the Iris plant based on sepal and petal measurements. This paper explores the application of Support Vector Machines (SVM) combined with Principal Component Analysis (PCA) for dimensionality reduction to enhance classification accuracy and interpretability. Performance metrics, including precision, recall, F1 score, and confusion matrices, are analyzed to evaluate model effectiveness. The study demonstrates that while SVM achieves high classification accuracy, Decision Trees provide greater interpretability, which is valuable for practical applications in biological classification.*\n\n## Introduction\n\nBioinformatics is an emerging and rapidly evolving field focused on extracting meaningful insights from biological data using computational techniques. Fundamental challenges in bioinformatics, such as protein structure prediction, sequence alignment, and phylogenetic inference, are often computationally complex and classified as NP-hard problems. Machine learning (ML) methods, including Artificial Neural Networks (ANN), Fuzzy Logic, Genetic Algorithms, and Support Vector Machines (SVM), have shown promise in addressing these challenges.\n\nThis study focuses on classifying Iris species using measurements of sepal length, sepal width, petal length, and petal width. The Iris dataset, introduced by R.A. Fisher in 1936, has become a standard benchmark for classification models. The goal is to evaluate the effectiveness of SVM for classification and explore the impact of dimensionality reduction using PCA on model performance. Decision Tree models are also assessed for comparison, balancing accuracy with interpretability.\n\n## Data\n\nThe Iris dataset, obtained from the UCI Machine Learning Repository, consists of 150 instances representing three species of Iris plants:\n\n - Iris Setosa\n - Iris Versicolor\n - Iris Virginica\n\nEach species is represented by 50 samples. The dataset includes four numeric attributes:\n\n - Sepal length (cm)\n - Sepal width (cm)\n - Petal length (cm)\n - Petal width (cm)\n\n::: {#247d38fe .cell tbl-cap='Iris Measures' execution_count=3}\n``` {.python .cell-code}\ndata = get_data(\"iris\")\ndecoder = ['setosa', 'versicolor', 'virginica']\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal_length</th>\n      <th>sepal_width</th>\n      <th>petal_length</th>\n      <th>petal_width</th>\n      <th>species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>Iris-setosa</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe fifth attribute is the class label indicating the species. The dataset is complete, with no missing values or inconsistencies reported. While the Setosa species is separable from the other two species, Versicolor and Virginica exhibit significant overlap, making classification the challenge of the study.\nA copy of the exploratory data report can be found here [EDA Report](../../assets/Iris_EDA.html)\n\n::: {#9f23a5da .cell execution_count=4}\n``` {.python .cell-code}\nreport = data.profile_report(progress_bar=False, title=\"Iris EDA Report\")\nreport.to_file(\"../../assets/Iris_EDA.html\", silent = True)\n```\n:::\n\n\n## Methods\n\nPrincipal Component Analysis (PCA) is a widely used dimensionality reduction technique that can project data onto a lower-dimensional subspace while retaining the maximum variance. PCA helps reduce the computational complexity of the classification task, but more importantly in this instance, improves visualization.\n\nIn this study, PCA reduces the four-dimensional Iris data into two principal components:\n\n - First Principal Component: Accounts for the highest variance in the data.\n - Second Principal Component: Captures the next highest variance.\n\n::: {#a8403354 .cell execution_count=5}\n``` {.python .cell-code}\nenv_pca = setup(data, target=\"species\", train_size = .8, session_id = RANDOM_STATE, pca=True, pca_components = 2, verbose = False, normalize = True)\n```\n:::\n\n\n::: {#8fd5019c .cell tbl-cap='Reduced Dimension Measurements' execution_count=6}\n``` {.python .cell-code}\nenv_pca.X_transformed.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pca0</th>\n      <th>pca1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>83</th>\n      <td>-1.069254</td>\n      <td>-0.680773</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>2.308932</td>\n      <td>1.167804</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>-0.230247</td>\n      <td>-0.329121</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>-1.329566</td>\n      <td>0.578077</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>0.044678</td>\n      <td>-1.574244</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nBy plotting the data along these two principal components, a clear separation between the Setosa species and the other two species becomes visible. However, Versicolor and Virginica still exhibit considerable overlap.\n\n::: {#bb6faf68 .cell execution_count=7}\n``` {.python .cell-code}\npca = env_pca.pipeline.named_steps[\"pca\"].transformer\nplt.figure()\nsns.scatterplot(env_pca.X_transformed, x=\"pca0\", y=\"pca1\", hue=data[\"species\"]).set(title=\"IRIS Species Scatter Plot \\n Explained Variance: %.3f\" % pca.explained_variance_ratio_.sum())\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){}\n:::\n:::\n\n\n### Feature Importance\nWhile PCA is great for dimensionality reduction and visualization, we do lose some interpretability in how the scientific measurements are correlated to the target species. For this, we look at feature rank and feature importance charts to determine the measurements most affecting the separation between classes.\n\n::: {#5f714505 .cell execution_count=8}\n``` {.python .cell-code}\nenv_norm = setup(data, target=\"species\", train_size = .8, session_id = RANDOM_STATE, verbose = False, normalize=True)\n```\n:::\n\n\n::: {#1033b0f8 .cell layout-ncol='2' execution_count=9}\n``` {.python .cell-code}\nfig = rank2d(get_config(\"X_train\"), get_config(\"y_train\"))\nfig = yb.target.feature_correlation.feature_correlation(env_norm.X_train, env_norm.y_train, method='mutual_info-classification')\n```\n\n::: {.cell-output .cell-output-display}\n![Pairwise Feature Correlation](index_files/figure-html/cell-10-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![Target Correlation](index_files/figure-html/cell-10-output-2.png){}\n:::\n:::\n\n\n<br>\nStandard practice is to perform train test split on the transformed data to evaluate the model performance.\nWe will use the convention that splits the data in 80% training/validation and 20% test set.\nBecause the sample size is relatively small, we will use cross validation to prevent overfitting.\nThe advantages of this technique are the ability to limit overfitting on a relatively small dataset.\n\n::: {#45a309fe .cell execution_count=10}\n``` {.python .cell-code}\nfig = yb.target.class_balance(env_norm.y_train, env_norm.y_test, labels = decoder)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){}\n:::\n:::\n\n\n## Analysis\nThe models were evaluated using an 80/20 train-test split with five-fold cross-validation to mitigate overfitting. Evaluation metrics included:\n\n - Precision – The proportion of correct positive predictions.\n - Recall – The proportion of actual positives correctly identified.\n - F1 Score – The harmonic mean of precision and recall, providing a balanced measure of model accuracy.\n - Confusion Matrix - Illustrates misclassification patterns across species.\n\n### Classification by Support Vector Machine \n\nSupport Vector Machines (SVM) are effective for high-dimensional classification problems. SVM constructs a hyperplane in a multi-dimensional space that maximally separates the classes. In cases where the data is not linearly separable, SVM employs a kernel trick to map the data into a higher-dimensional space where linear separation becomes possible. \n\n::: {#2812c501 .cell tbl-cap='Cross Validation Results' execution_count=11}\n``` {.python .cell-code}\nsvm_model = create_model(\"svm\", tol=1e-3, alpha=.0012, verbose=False) # parameters can be found with tune_model\npull()\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Accuracy</th>\n      <th>AUC</th>\n      <th>Recall</th>\n      <th>Prec.</th>\n      <th>F1</th>\n      <th>Kappa</th>\n      <th>MCC</th>\n    </tr>\n    <tr>\n      <th>Fold</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.7500</td>\n      <td>0.0</td>\n      <td>0.7500</td>\n      <td>0.7556</td>\n      <td>0.7460</td>\n      <td>0.6250</td>\n      <td>0.6316</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.9167</td>\n      <td>0.0</td>\n      <td>0.9167</td>\n      <td>0.9333</td>\n      <td>0.9153</td>\n      <td>0.8750</td>\n      <td>0.8843</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.9167</td>\n      <td>0.0</td>\n      <td>0.9167</td>\n      <td>0.9333</td>\n      <td>0.9153</td>\n      <td>0.8750</td>\n      <td>0.8843</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.9167</td>\n      <td>0.0</td>\n      <td>0.9167</td>\n      <td>0.9333</td>\n      <td>0.9153</td>\n      <td>0.8750</td>\n      <td>0.8843</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0000</td>\n      <td>0.0</td>\n      <td>1.0000</td>\n      <td>1.0000</td>\n      <td>1.0000</td>\n      <td>1.0000</td>\n      <td>1.0000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.8333</td>\n      <td>0.0</td>\n      <td>0.8333</td>\n      <td>0.8333</td>\n      <td>0.8333</td>\n      <td>0.7500</td>\n      <td>0.7500</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1.0000</td>\n      <td>0.0</td>\n      <td>1.0000</td>\n      <td>1.0000</td>\n      <td>1.0000</td>\n      <td>1.0000</td>\n      <td>1.0000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1.0000</td>\n      <td>0.0</td>\n      <td>1.0000</td>\n      <td>1.0000</td>\n      <td>1.0000</td>\n      <td>1.0000</td>\n      <td>1.0000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.9167</td>\n      <td>0.0</td>\n      <td>0.9167</td>\n      <td>0.9333</td>\n      <td>0.9153</td>\n      <td>0.8750</td>\n      <td>0.8843</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.8333</td>\n      <td>0.0</td>\n      <td>0.8333</td>\n      <td>0.8889</td>\n      <td>0.8222</td>\n      <td>0.7500</td>\n      <td>0.7833</td>\n    </tr>\n    <tr>\n      <th>Mean</th>\n      <td>0.9083</td>\n      <td>0.0</td>\n      <td>0.9083</td>\n      <td>0.9211</td>\n      <td>0.9063</td>\n      <td>0.8625</td>\n      <td>0.8702</td>\n    </tr>\n    <tr>\n      <th>Std</th>\n      <td>0.0786</td>\n      <td>0.0</td>\n      <td>0.0786</td>\n      <td>0.0744</td>\n      <td>0.0805</td>\n      <td>0.1179</td>\n      <td>0.1141</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### Comparison to Decision Tree\n\nA Decision Tree classifier was used as a benchmark to compare with the SVM model. Decision Trees provide an interpretable model by recursively splitting the data based on the most informative features. The tree structure allows for straightforward interpretation of how classifications are made.\n\n::: {#bfdd195d .cell execution_count=12}\n``` {.python .cell-code}\ntree_model = create_model(\"dt\", max_depth=3, verbose = False)\nwinning_model = compare_models(include=[svm_model, tree_model], verbose = False)\npull()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Accuracy</th>\n      <th>AUC</th>\n      <th>Recall</th>\n      <th>Prec.</th>\n      <th>F1</th>\n      <th>Kappa</th>\n      <th>MCC</th>\n      <th>TT (Sec)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>Decision Tree Classifier</td>\n      <td>0.9500</td>\n      <td>0.9677</td>\n      <td>0.9500</td>\n      <td>0.9600</td>\n      <td>0.9492</td>\n      <td>0.9250</td>\n      <td>0.9306</td>\n      <td>0.005</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>SVM - Linear Kernel</td>\n      <td>0.9083</td>\n      <td>0.0000</td>\n      <td>0.9083</td>\n      <td>0.9211</td>\n      <td>0.9063</td>\n      <td>0.8625</td>\n      <td>0.8702</td>\n      <td>0.008</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#fig-metrics layout-ncol=2}\n\n::: {#e0603ac0 .cell execution_count=13}\n``` {.python .cell-code}\nfig = plot_model(svm_model, plot = \"confusion_matrix\", plot_kwargs = {\"classes\": decoder})\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-2.png){}\n:::\n:::\n\n\n::: {#31e56b9d .cell execution_count=14}\n``` {.python .cell-code}\nfig = plot_model(tree_model, plot = \"confusion_matrix\", plot_kwargs = {\"classes\": decoder})\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-2.png){}\n:::\n:::\n\n\n::: {#75c49c1f .cell execution_count=15}\n``` {.python .cell-code}\nfig = plot_model(svm_model, plot = \"class_report\", plot_kwargs = {\"classes\": decoder})\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-16-output-2.png){}\n:::\n:::\n\n\n::: {#aa690cec .cell execution_count=16}\n``` {.python .cell-code}\nfig = plot_model(tree_model, plot = \"class_report\", plot_kwargs = {\"classes\": decoder})\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-2.png){}\n:::\n:::\n\n\n:::\n\nThe Decision Tree model achieved comparable performance, almost identical to the Support Vector Machine. This is a common phenomenon in machine learning applications and often greatly ignored by auto=ml packages. Decision Trees, provide a more transparent decision-making process, which is valuable for field applications where understanding the classification process is essential.\n\n::: {#6de377bf .cell execution_count=17}\n``` {.python .cell-code}\nmessage = save_model(svm_model, \"models/svm-model\", verbose = False)\nmessage = save_model(tree_model, \"models/tree-model\", verbose = False)\n```\n:::\n\n\n### Model Interpretability\n\nA simple decision tree can be printed out as a flowchart or a series of branching yes/no questions, making it easy to use in field work where quick decisions are necessary. Each node in the tree represents a question based on a specific feature, such as \"Is petal length greater than 2.5 cm?\" The branches lead to subsequent questions or to a classification decision, such as identifying the plant as Setosa or Versicolor. This format allows field researchers to visually trace the decision path step-by-step, even without access to computational tools. The straightforward structure of a decision tree makes it easy to understand and follow, enabling non-experts to accurately classify samples based on observable characteristics. The transparency and simplicity of the printed decision tree make it particularly valuable for practical applications in biological fieldwork.\n\n::: {#2372366b .cell execution_count=18}\n``` {.python .cell-code}\n# Build a model on the final data for deployment.\nenv_prod = setup(data, target=\"species\", train_size = .8, session_id = RANDOM_STATE, verbose = False) # trees don't need normalization\nfinal_tree = finalize_model(tree_model)\nmessage = save_model(final_tree, \"models/production-model\", verbose = False)\n```\n:::\n\n\n::: {#4b766fb7 .cell execution_count=19}\n``` {.python .cell-code}\n# Visualize predictions\nsample = env_prod.X_transformed.iloc[120] \nfinal_tree_model = final_tree.named_steps['actual_estimator']\nviz_model =  dtreeviz.model(final_tree_model, X_train=env_prod.X_transformed, y_train = env_prod.y_transformed, feature_names = list(env_prod.X_transformed), target_name = \"species name\", class_names = decoder)\nviz_model.view(x = sample, fontname=\"DejaVu Sans\")\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n![](index_files/figure-html/cell-20-output-1.svg){}\n:::\n:::\n\n\nIn-case displaying the entire tree is not desirable especially in regulatory environments, we can instead display the list of features and their importance related to the decision. \n\n::: {#5255da60 .cell execution_count=20}\n``` {.python .cell-code}\nviz_model.instance_feature_importance(sample, fontname=\"DejaVu Sans\")\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-21-output-1.png){}\n:::\n:::\n\n\n## Conclusion\n\n### Discussion\n\nMisclassification between Versicolor and Virginica reflects the biological similarities in their sepal and petal measurements. Future research could explore additional botanical features or incorporate other ML techniques, such as ensemble methods or deep learning, to improve discrimination between these species.\n\nSVM remains a powerful classification tool for complex, high-dimensional datasets. However, the increased interpretability of Decision Trees suggests that they may be more suitable for practical, real-world applications in biological research.\n\n### Conclusion\n\nThis study demonstrates the effectiveness of SVM in classifying Iris species based on sepal and petal measurements. PCA successfully reduced the data dimensionality, improving visualization. SVM achieves high accuracy, however, the interpretability of Decision Trees makes them a valuable alternative for practical applications where model transparency is critical.\n\n### Future Work\n\nFuture work could explore:\n\n - Incorporating additional morphological features to improve classification between Versicolor and Virginica.\n - Testing ensemble methods such as Random Forest or Gradient Boosted Trees for enhanced accuracy.\n\n## Appendix\n\nThe entire website, including this example project is located at [https://github.com/joshuacharleshyatt/personal](https://github.com/joshuacharleshyatt/personal)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}