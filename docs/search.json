[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "COVID Reflection\n\n\n\nPaper\n\n\nPython\n\n\nDataviz\n\n\nCOVID\n\n\n\n\nMar 18, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample Paper\n\n\n\nPaper\n\n\nPython\n\n\nDataviz\n\n\n\n\nMar 8, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/COVID/index.html",
    "href": "projects/COVID/index.html",
    "title": "COVID Reflection",
    "section": "",
    "text": "Code\nimport pandas as pd\nfrom forecast import optimize_fit, SIR \nimport matplotlib.pyplot as plt\nimport numpy as np\nAbstract Goes Here"
  },
  {
    "objectID": "projects/COVID/index.html#introduction",
    "href": "projects/COVID/index.html#introduction",
    "title": "COVID Reflection",
    "section": "Introduction",
    "text": "Introduction\nBack in 2020 on July 29th, I published a long term COVID prediction model based on a simple and well known Epidemiology model called SIR. The model makes long term predictions on what happens to a population when a new infectious disease is introduced. At the time of writing, this wasn’t a popular model to pursue since it was harder to predict and prove short term COVID case counts. Now in 2025, I thought it would be interesting to see how good this kind of modeling was and how it could have been improved in hindsight."
  },
  {
    "objectID": "projects/COVID/index.html#data",
    "href": "projects/COVID/index.html#data",
    "title": "COVID Reflection",
    "section": "Data",
    "text": "Data\nThe following data represents counties with various population, mobility, and disease case numbers. The data is provided by multiple sources, including John Hopkins University, Apple, Google and the CDC. COVID 19 case numbers are relatively inaccurate measurements of the number of infected people as testing has been inconsistent in the US and the federal guidelines for reporting were not established early enough during collection of this data.\n\n\nCode\ncovid_2020 = pd.read_csv(\"covid_counties_7_2020.csv\", low_memory=False)[['date', 'State', 'County.Name', 'countyFIPS', 'cases', 'deaths', 'population', 'new_cases', 'new_deaths']]\ncovid_2020[100:110].head(5)\n\n\n\n\n\n\n\n\n\ndate\nState\nCounty.Name\ncountyFIPS\ncases\ndeaths\npopulation\nnew_cases\nnew_deaths\n\n\n\n\n100\n2020-05-01\nAlabama\nAutauga\n1001\n42\n3.0\n55869.0\n0.0\n0.0\n\n\n101\n2020-05-02\nAlabama\nAutauga\n1001\n45\n3.0\n55869.0\n3.0\n0.0\n\n\n102\n2020-05-03\nAlabama\nAutauga\n1001\n48\n3.0\n55869.0\n3.0\n0.0\n\n\n103\n2020-05-04\nAlabama\nAutauga\n1001\n53\n3.0\n55869.0\n5.0\n0.0\n\n\n104\n2020-05-05\nAlabama\nAutauga\n1001\n53\n3.0\n55869.0\n0.0\n0.0\n\n\n\n\n\n\n\nWe will study the effects of the virus at the National level and simplify the data a bit more.\n\n\nCode\ncovid_usa = covid_2020[['date','cases','deaths','population']].groupby(by=['date']).sum()\ncovid_usa.head(5)\n\n\n\n\n\n\n\n\n\ncases\ndeaths\npopulation\n\n\ndate\n\n\n\n\n\n\n\n2020-01-22\n1\n0.0\n328021328.0\n\n\n2020-01-23\n1\n0.0\n328021328.0\n\n\n2020-01-24\n2\n0.0\n328021328.0\n\n\n2020-01-25\n2\n0.0\n328021328.0\n\n\n2020-01-26\n5\n0.0\n328021328.0"
  },
  {
    "objectID": "projects/COVID/index.html#methods",
    "href": "projects/COVID/index.html#methods",
    "title": "COVID Reflection",
    "section": "Methods",
    "text": "Methods\nA simple mathematical description of the spread of a disease in a population is the so-called SIR model, which divides the (fixed) population of N individuals into three “compartments” which may vary as a function of time, t:\n\nS(t) are those susceptible but not yet infected with the disease\nI(t) is the number of infectious individuals\nR(t) are those individuals who have recovered from the disease and now have immunity to it.\n\nThe SIR model describes the change in the population of each of these compartments in terms of two parameters, β and γ. β describes the effective contact rate of the disease: an infected individual comes into contact with β other individuals per unit time (of which the fraction that are susceptible to contracting the disease is S/N). γ is the mean recovery rate: that is, 1/γ is the mean period of time during which an infected individual can pass it on. γ can often be calculated through statistical means, while β can be easily found at the end of the epidemic.\nSIR models uses ODE to evaluate the change in infected to recovered cases in a population. The system of ODE is solved for integer time t and the output is a model of each function of SIR with respect to time. It can be shown that the system of ODE is given by the following\n\\[\\frac{dS}{dt} = - \\beta * I(t) * S(t) / N(t) \\] \\[\\frac{dI}{dt} = \\beta * I(t) * S(t) / N(t) - \\gamma * I(t)\\] \\[\\frac{dR}{dt} = \\gamma * I(t)\\]\n\n\nCode\nγ_inverse = 10\nγ = 1/γ_inverse\n\ncovid_usa['recovered_cases'] = covid_usa.shift(periods = γ_inverse)['cases'] # R\ncovid_usa['infected'] = covid_usa['cases'] - covid_usa['recovered_cases'] # I\npopulation = int(covid_usa['population'].mean()) # N\n\ncovid_usa.tail(5)\n\n\n\n\n\n\n\n\n\ncases\ndeaths\npopulation\nrecovered_cases\ninfected\n\n\ndate\n\n\n\n\n\n\n\n\n\n2020-07-21\n3871366\n140654.0\n328021328.0\n3572226.0\n299140.0\n\n\n2020-07-22\n3949024\n141922.0\n328021328.0\n3637066.0\n311958.0\n\n\n2020-07-23\n4006793\n142818.0\n328021328.0\n3700379.0\n306414.0\n\n\n2020-07-24\n4080745\n143960.0\n328021328.0\n3778177.0\n302568.0\n\n\n2020-07-25\n4145882\n144880.0\n328021328.0\n3845894.0\n299988.0\n\n\n\n\n\n\n\nThe fundamental problem of this model is finding β during the pandemic, of which there is hope in defining c = β/γ = # of close contacts infections a person will have during their contagious period. This can be statistically calculated through contact tracing and testing and can also be found at the end of the pandemic. Unfortunately, this pandemic has proven hard to test and trace accurately, making contact tracing difficult if not impossible to calculate. The approach used in this study for finding parameter β is by fitting the SIR model to historic data using quasi Newtonian optimization methods.\nUsing this history based method for calculating β, the parameter γ must also be evaluated. Fortunately, from many resources such as John Hopkins University and now the CDC, 1/γ is estimated as 10 days (meaning an infected person is contagious for 10 days on average). Using this information, COVID 19 case counts can be converted to active (meaning contagious ) case counts by using a sliding window of 1/γ = 10 days. Thus our study requires 70 days of case history to train and model 60 days worth of day. For this, We assume new case counts are the start of the contagious period of 1/γ.\n\n\nCode\nβ0 = 0.16 # a good starting guess to find β using gradient descent\nwindow = 30\nhistoric = covid_usa.iloc[-(window+2)]\nβ = optimize_fit(β0, population, γ, historic.infected, historic.recovered_cases, covid_usa[-(window+1):-1].infected.values, window)\nprint(f\"β = {β}\")\n\n\nβ = 0.12333714490017725"
  },
  {
    "objectID": "projects/COVID/index.html#analysis",
    "href": "projects/COVID/index.html#analysis",
    "title": "COVID Reflection",
    "section": "Analysis",
    "text": "Analysis\n\n\nCode\nforecast_horizon = 500\nsir_model = SIR(β,γ,historic.population)\ns,i,r = sir_model.predict(historic.infected, historic.recovered_cases, forecast_horizon)\n\n\n\n\nCode\nt = np.arange(np.datetime64(historic.name), np.datetime64(historic.name) + forecast_horizon)\n\nfig = plt.figure(facecolor='w', figsize=(10, 5))\nax = fig.add_subplot(111, facecolor='#dddddd', axisbelow=True)\nax.plot(t, s, 'b', alpha=0.5, lw=2, label='Susceptible')\nax.plot(t, i, 'r', alpha=0.5, lw=2, label='Active Cases')\nax.plot(t, r, 'g', alpha=0.5, lw=2, label='Recovered with immunity')\nax.set_xlabel('Dates')\nax.set_ylabel('Forecasted Values (Millions)')\nax.yaxis.set_tick_params(length=5)\nax.xaxis.set_tick_params(length=5, rotation=20)\n\n# Add descriptions to chart\n#ax.axvline(x=np.datetime64(\"2020-07-20\"), color='gray', linestyle='--', linewidth=2, label=\"Event Date\")\n#ax.text(np.datetime64(\"2020-07-20\"), 10**8, \"Key Event\", color='gray', fontsize=12, ha='right', va='top')\n\nlegend = ax.legend()\nlegend.get_frame().set_alpha(0.5)\nfor spine in ('top', 'right', 'bottom', 'left'):\n    ax.spines[spine].set_visible(False)\nax.set_title(\"COVID Forecast\")\nplt.savefig(\"covid_predictions.png\")"
  },
  {
    "objectID": "qualifications.html",
    "href": "qualifications.html",
    "title": "Joshua Hyatt",
    "section": "",
    "text": "I am a pragmatic and results-oriented Data Scientist with over 10 years of experience delivering production-grade machine learning solutions across the automotive, finance, and education sectors. Passionate about solving real-world problems using statistical modeling, computer vision, and NLP. Proven ability to lead cross-functional projects, to design and implement robust MLOps pipelines using Databricks, Docker, and CI/CD best practices. Adept at working in collaborative environments and translating complex data into strategic business outcomes."
  },
  {
    "objectID": "qualifications.html#professional-experience",
    "href": "qualifications.html#professional-experience",
    "title": "Joshua Hyatt",
    "section": "Professional Experience",
    "text": "Professional Experience\n\n\n\n\n\n%%{init: {'theme':'neutral'}}%%\ngantt\n  dateFormat YYYY-MM\n  axisformat %y\n  section Education\n    Bachelor          :done, e1, 2007-08, 4y\n    Masters           :done, e2, after e1, 2y\n  section Work Experience\n    Pellissippi State :done, w1, after e2, 2y\n    Elavon            :done, w2, after w1, 3y\n    DENSO             :active, w3, after w2, 8y\n\n\n\n\n\n\n\nData Scientist III\n\n\n\n\n\n\nDENSO International America, INC. Maryville, TN July 2018 – Present\n\n\n\n\nCollaborated with MIT to simulate and analyze supply chain resilience using Gurobi and GIS datasets, informing risk mitigation plans for natural disasters.\nDeveloped a computer vision defect detection system using segmentation-based CNNs and convolutional autoencoders, deployed via Flask, containerized, and integrated for robotic process control using OPCUA, reducing defect rates and human inspection costs.\nLed deployment of Databricks-based pipelines for time series analysis and predictive maintenance, leveraging Data Quality Pipelines, MLflow for experiment tracking, and Unity Catalog for data and model asset management.\nBuilt NLP models using sentence embeddings and UMAP for clustering procurement descriptions, improving spend visibility and optimizing purchasing processes.\nLed the Data Science Internship Program, handling hiring, project scoping, daily standups, and mentorship; delivered production-quality NLP and analytics solutions while establishing training resources and workflows for future prospects.\nAccurately forecasted the COVID-19 case peak using an SEIR model 16 months in advance (8-day margin of error), enabling proactive workforce planning and minimizing disruption to manufacturing operations.\nEstablished an Enterprise CI/CD pipeline using Azure Devops with Databricks, as well as GitLab with Docker to automate testing and deployment of ML applications with API endpoints\n\n\n\n\n\nBusiness Analyst II ← DBA ← Fraud Analyst\n\n\n\n\n\n\nElavon Knoxville, TN Nov 2015 – July 2018\n\n\n\n\nImproved fraud detection accuracy and work queue triage, resulting in an additional $200K in annual fraud prevention.\nBuilt a web scraping tool that halved account investigation time, with estimated annual savings between $25K–$50K.\nFounded and led the Dedicated Solutions Team to automate reporting and data quality monitoring for internal business units.\nRecognised with the Pinnacle Award (top 10% company-wide performance) in 2016.\n\n\n\n\n\nAdjunct Mathematics Professor\n\n\n\n\n\n\nPellissippi State Community College Knoxville, TN Aug 2013 – Nov 2015\n\n\n\n\nTaught Statistics, Algebra, and Business Calculus, supporting diverse learners and maintaining strong student outcomes."
  },
  {
    "objectID": "qualifications.html#education",
    "href": "qualifications.html#education",
    "title": "Joshua Hyatt",
    "section": "Education",
    "text": "Education\n\n\n\n\n\n\nMasters of Science in Mathematics\nUniversity of Tennessee - Knoxville, TN\nGPA: 3.97/4.0\n\n\n\n\nBachelor of Science in Mathematics\nMurray State University - Murray, KY\nGPA: 3.81/4.0"
  },
  {
    "objectID": "qualifications.html#expertise",
    "href": "qualifications.html#expertise",
    "title": "Joshua Hyatt",
    "section": "Expertise",
    "text": "Expertise\n\n\n\n\n\n%%{init: {'theme':'neutral'}}%%\nmindmap\n  root((Skills))\n    Analytics\n      Natural Language Processing\n      Anomaly Detection\n      Convolutional Neural Networks\n      Business Intelligence\n      Literate Progamming\n    Programming\n      Python\n      GIT\n      SQL\n      Linux\n    Deployment\n      Docker\n      CI/CD\n      Automated Testing\n\n\n\n\n\n\n\n\n\n\n\n\nDetailed Descriptions\n\n\n\n\n\n\nAnalytics\n\nNatural Language Processing (Word and Sentence embedding, Sentiment Analysis)\nAnomaly Detection (Auto-Encoders, One Class SVM, Local Outlier Factor, PCA)\nConvolutional Neural Networks (Auto-Encoders, U-net, Image Embedding)\nBI (Tableau, DOMO)\nLiterate Programming (Jupyter Notebooks, Quarto, Codebraid)\n\n\n\nProgramming\n\nPython (NumPy, Pandas, gurobipy, GeoPandas, Scikit-learn, Tensorflow, SciPy, Flask, NLTK, SQLAlchemy, NetworkX, matplotlib, BaseMap, shapely, Beautiful Soup)\nGIT\nSQL (Postgres, MSSQL, Cassandra, etc)\nLinux (including terminal)\n\n\n\nDeployment\n\nDocker Containerization\nCI/CD Pipeline (Gitlab Runner)\nAutomated Testing (Pytest, Selenium Web Testing)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joshua Hyatt",
    "section": "",
    "text": "I am a pragmatic and results-oriented Data Scientist with over 10 years of experience delivering production-grade machine learning solutions across the automotive, finance, and education sectors. Passionate about solving real-world problems using statistical modeling, computer vision, and NLP. Proven ability to lead cross-functional projects, to design and implement robust MLOps pipelines using Databricks, Docker, and CI/CD best practices. Adept at working in collaborative environments and translating complex data into strategic business outcomes."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Joshua Hyatt",
    "section": "",
    "text": "I am a pragmatic and results-oriented Data Scientist with over 10 years of experience delivering production-grade machine learning solutions across the automotive, finance, and education sectors. Passionate about solving real-world problems using statistical modeling, computer vision, and NLP. Proven ability to lead cross-functional projects, to design and implement robust MLOps pipelines using Databricks, Docker, and CI/CD best practices. Adept at working in collaborative environments and translating complex data into strategic business outcomes."
  },
  {
    "objectID": "projects/Child Nutrition/index.html",
    "href": "projects/Child Nutrition/index.html",
    "title": "Child Malnutrition Dashboard",
    "section": "",
    "text": "Dashboard"
  },
  {
    "objectID": "projects/Template/index.html",
    "href": "projects/Template/index.html",
    "title": "Example Paper",
    "section": "",
    "text": "Code\nimport io\nfrom sklearn import get_config\nfrom config import save_system\nfrom encryption import RandomState\nimport tomli\nfrom contextlib import redirect_stdout\n\n# remove stupid advert\nf = io.StringIO()\nwith redirect_stdout(f):\n  import ydata_profiling as yd\n\nfrom pycaret.datasets import get_data\nfrom pycaret.classification.functional import setup, get_config, compare_models, create_model, pull, finalize_model, save_model, plot_model\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams['font.family'] = 'DeJavu Serif'\nplt.rcParams['font.serif'] = ['Times New Roman']\nfrom yellowbrick.features import rank2d\nimport yellowbrick as yb\nimport dtreeviz\nCode\nsave_system()\n\nwith open(\"../../pyproject.toml\", mode=\"rb\") as fp:\n  config = tomli.load(fp)\n\nPROJECT_NAME = config[\"tool\"][\"poetry\"][\"name\"]\nRANDOM_STATE = RandomState(PROJECT_NAME).state"
  },
  {
    "objectID": "projects/Template/index.html#introduction",
    "href": "projects/Template/index.html#introduction",
    "title": "Example Paper",
    "section": "Introduction",
    "text": "Introduction\nBioinformatics is an emerging and rapidly evolving field focused on extracting meaningful insights from biological data using computational techniques. Fundamental challenges in bioinformatics, such as protein structure prediction, sequence alignment, and phylogenetic inference, are often computationally complex and classified as NP-hard problems. Machine learning (ML) methods, including Artificial Neural Networks (ANN), Fuzzy Logic, Genetic Algorithms, and Support Vector Machines (SVM), have shown promise in addressing these challenges.\nThis study focuses on classifying Iris species using measurements of sepal length, sepal width, petal length, and petal width. The Iris dataset, introduced by R.A. Fisher in 1936, has become a standard benchmark for classification models. The goal is to evaluate the effectiveness of SVM for classification and explore the impact of dimensionality reduction using PCA on model performance. Decision Tree models are also assessed for comparison, balancing accuracy with interpretability."
  },
  {
    "objectID": "projects/Template/index.html#data",
    "href": "projects/Template/index.html#data",
    "title": "Example Paper",
    "section": "Data",
    "text": "Data\nThe Iris dataset, obtained from the UCI Machine Learning Repository, consists of 150 instances representing three species of Iris plants:\n\nIris Setosa\nIris Versicolor\nIris Virginica\n\nEach species is represented by 50 samples. The dataset includes four numeric attributes:\n\nSepal length (cm)\nSepal width (cm)\nPetal length (cm)\nPetal width (cm)\n\n\n\nCode\ndata = get_data(\"iris\")\ndecoder = ['setosa', 'versicolor', 'virginica']\n\n\n\n\n\n\nIris Measures\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\nThe fifth attribute is the class label indicating the species. The dataset is complete, with no missing values or inconsistencies reported. While the Setosa species is separable from the other two species, Versicolor and Virginica exhibit significant overlap, making classification the challenge of the study. A copy of the exploratory data report can be found here EDA Report\n\n\nCode\nreport = data.profile_report(progress_bar=False, title=\"Iris EDA Report\")\nreport.to_file(\"../../assets/Iris_EDA.html\", silent = True)"
  },
  {
    "objectID": "projects/Template/index.html#methods",
    "href": "projects/Template/index.html#methods",
    "title": "Example Paper",
    "section": "Methods",
    "text": "Methods\nPrincipal Component Analysis (PCA) is a widely used dimensionality reduction technique that can project data onto a lower-dimensional subspace while retaining the maximum variance. PCA helps reduce the computational complexity of the classification task, but more importantly in this instance, improves visualization.\nIn this study, PCA reduces the four-dimensional Iris data into two principal components:\n\nFirst Principal Component: Accounts for the highest variance in the data.\nSecond Principal Component: Captures the next highest variance.\n\n\n\nCode\nenv_pca = setup(data, target=\"species\", train_size = .8, session_id = RANDOM_STATE, pca=True, pca_components = 2, verbose = False, normalize = True)\n\n\n\n\nCode\nenv_pca.X_transformed.head()\n\n\n\n\n\n\nReduced Dimension Measurements\n\n\n\npca0\npca1\n\n\n\n\n83\n-1.069254\n-0.680773\n\n\n19\n2.308932\n1.167804\n\n\n84\n-0.230247\n-0.329121\n\n\n52\n-1.329566\n0.578077\n\n\n81\n0.044678\n-1.574244\n\n\n\n\n\n\n\nBy plotting the data along these two principal components, a clear separation between the Setosa species and the other two species becomes visible. However, Versicolor and Virginica still exhibit considerable overlap.\n\n\nCode\npca = env_pca.pipeline.named_steps[\"pca\"].transformer\nplt.figure()\nsns.scatterplot(env_pca.X_transformed, x=\"pca0\", y=\"pca1\", hue=data[\"species\"]).set(title=\"IRIS Species Scatter Plot \\n Explained Variance: %.3f\" % pca.explained_variance_ratio_.sum())\n\n\n\n\n\n\n\n\n\n\nFeature Importance\nWhile PCA is great for dimensionality reduction and visualization, we do lose some interpretability in how the scientific measurements are correlated to the target species. For this, we look at feature rank and feature importance charts to determine the measurements most affecting the separation between classes.\n\n\nCode\nenv_norm = setup(data, target=\"species\", train_size = .8, session_id = RANDOM_STATE, verbose = False, normalize=True)\n\n\n\nCode\nfig = rank2d(get_config(\"X_train\"), get_config(\"y_train\"))\nfig = yb.target.feature_correlation.feature_correlation(env_norm.X_train, env_norm.y_train, method='mutual_info-classification')\n\n\n\n\n\n\n\nPairwise Feature Correlation\n\n\n\n\n\n\n\nTarget Correlation\n\n\n\n\n\n Standard practice is to perform train test split on the transformed data to evaluate the model performance. We will use the convention that splits the data in 80% training/validation and 20% test set. Because the sample size is relatively small, we will use cross validation to prevent overfitting. The advantages of this technique are the ability to limit overfitting on a relatively small dataset.\n\n\nCode\nfig = yb.target.class_balance(env_norm.y_train, env_norm.y_test, labels = decoder)"
  },
  {
    "objectID": "projects/Template/index.html#analysis",
    "href": "projects/Template/index.html#analysis",
    "title": "Example Paper",
    "section": "Analysis",
    "text": "Analysis\nThe models were evaluated using an 80/20 train-test split with five-fold cross-validation to mitigate overfitting. Evaluation metrics included:\n\nPrecision – The proportion of correct positive predictions.\nRecall – The proportion of actual positives correctly identified.\nF1 Score – The harmonic mean of precision and recall, providing a balanced measure of model accuracy.\nConfusion Matrix - Illustrates misclassification patterns across species.\n\n\nClassification by Support Vector Machine\nSupport Vector Machines (SVM) are effective for high-dimensional classification problems. SVM constructs a hyperplane in a multi-dimensional space that maximally separates the classes. In cases where the data is not linearly separable, SVM employs a kernel trick to map the data into a higher-dimensional space where linear separation becomes possible.\n\n\nCode\nsvm_model = create_model(\"svm\", tol=1e-3, alpha=.0012, verbose=False) # parameters can be found with tune_model\npull()\n\n\n\n\n\n\nCross Validation Results\n\n\n\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n\n\n\n\n\n\n\n\n\n\n\n0\n0.7500\n0.0\n0.7500\n0.7556\n0.7460\n0.6250\n0.6316\n\n\n1\n0.9167\n0.0\n0.9167\n0.9333\n0.9153\n0.8750\n0.8843\n\n\n2\n0.9167\n0.0\n0.9167\n0.9333\n0.9153\n0.8750\n0.8843\n\n\n3\n0.9167\n0.0\n0.9167\n0.9333\n0.9153\n0.8750\n0.8843\n\n\n4\n1.0000\n0.0\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n5\n0.8333\n0.0\n0.8333\n0.8333\n0.8333\n0.7500\n0.7500\n\n\n6\n1.0000\n0.0\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n7\n1.0000\n0.0\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n8\n0.9167\n0.0\n0.9167\n0.9333\n0.9153\n0.8750\n0.8843\n\n\n9\n0.8333\n0.0\n0.8333\n0.8889\n0.8222\n0.7500\n0.7833\n\n\nMean\n0.9083\n0.0\n0.9083\n0.9211\n0.9063\n0.8625\n0.8702\n\n\nStd\n0.0786\n0.0\n0.0786\n0.0744\n0.0805\n0.1179\n0.1141\n\n\n\n\n\n\n\n\n\nComparison to Decision Tree\nA Decision Tree classifier was used as a benchmark to compare with the SVM model. Decision Trees provide an interpretable model by recursively splitting the data based on the most informative features. The tree structure allows for straightforward interpretation of how classifications are made.\n\n\nCode\ntree_model = create_model(\"dt\", max_depth=3, verbose = False)\nwinning_model = compare_models(include=[svm_model, tree_model], verbose = False)\npull()\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\nTT (Sec)\n\n\n\n\n1\nDecision Tree Classifier\n0.9500\n0.9677\n0.9500\n0.9600\n0.9492\n0.9250\n0.9306\n0.005\n\n\n0\nSVM - Linear Kernel\n0.9083\n0.0000\n0.9083\n0.9211\n0.9063\n0.8625\n0.8702\n0.008\n\n\n\n\n\n\n\n\nCode\nfig = plot_model(svm_model, plot = \"confusion_matrix\", plot_kwargs = {\"classes\": decoder})\nfig = plot_model(tree_model, plot = \"confusion_matrix\", plot_kwargs = {\"classes\": decoder})\nfig = plot_model(svm_model, plot = \"class_report\", plot_kwargs = {\"classes\": decoder})\nfig = plot_model(tree_model, plot = \"class_report\", plot_kwargs = {\"classes\": decoder})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\nThe Decision Tree model achieved comparable performance, almost identical to the Support Vector Machine. This is a common phenomenon in machine learning applications and often greatly ignored by auto=ml packages. Decision Trees, provide a more transparent decision-making process, which is valuable for field applications where understanding the classification process is essential.\n\n\nCode\nmessage = save_model(svm_model, \"models/svm-model\", verbose = False)\nmessage = save_model(tree_model, \"models/tree-model\", verbose = False)\n\n\n\n\nModel Interpretability\nA simple decision tree can be printed out as a flowchart or a series of branching yes/no questions, making it easy to use in field work where quick decisions are necessary. Each node in the tree represents a question based on a specific feature, such as “Is petal length greater than 2.5 cm?” The branches lead to subsequent questions or to a classification decision, such as identifying the plant as Setosa or Versicolor. This format allows field researchers to visually trace the decision path step-by-step, even without access to computational tools. The straightforward structure of a decision tree makes it easy to understand and follow, enabling non-experts to accurately classify samples based on observable characteristics. The transparency and simplicity of the printed decision tree make it particularly valuable for practical applications in biological fieldwork.\n\n\nCode\n# Build a model on the final data for deployment.\nenv_prod = setup(data, target=\"species\", train_size = .8, session_id = RANDOM_STATE, verbose = False) # trees don't need normalization\nfinal_tree = finalize_model(tree_model)\nmessage = save_model(final_tree, \"models/production-model\", verbose = False)\n\n\n\n\nCode\n# Visualize predictions\nsample = env_prod.X_transformed.iloc[120] \nfinal_tree_model = final_tree.named_steps['actual_estimator']\nviz_model =  dtreeviz.model(final_tree_model, X_train=env_prod.X_transformed, y_train = env_prod.y_transformed, feature_names = list(env_prod.X_transformed), target_name = \"species name\", class_names = decoder)\nviz_model.view(x = sample, fontname=\"DejaVu Sans\")\n\n\n\n\n\n\n\n\n\nIn-case displaying the entire tree is not desirable especially in regulatory environments, we can instead display the list of features and their importance related to the decision.\n\n\nCode\nviz_model.instance_feature_importance(sample, fontname=\"DejaVu Sans\")"
  },
  {
    "objectID": "projects/Template/index.html#conclusion",
    "href": "projects/Template/index.html#conclusion",
    "title": "Example Paper",
    "section": "Conclusion",
    "text": "Conclusion\n\nDiscussion\nMisclassification between Versicolor and Virginica reflects the biological similarities in their sepal and petal measurements. Future research could explore additional botanical features or incorporate other ML techniques, such as ensemble methods or deep learning, to improve discrimination between these species.\nSVM remains a powerful classification tool for complex, high-dimensional datasets. However, the increased interpretability of Decision Trees suggests that they may be more suitable for practical, real-world applications in biological research.\n\n\nConclusion\nThis study demonstrates the effectiveness of SVM in classifying Iris species based on sepal and petal measurements. PCA successfully reduced the data dimensionality, improving visualization. SVM achieves high accuracy, however, the interpretability of Decision Trees makes them a valuable alternative for practical applications where model transparency is critical.\n\n\nFuture Work\nFuture work could explore:\n\nIncorporating additional morphological features to improve classification between Versicolor and Virginica.\nTesting ensemble methods such as Random Forest or Gradient Boosted Trees for enhanced accuracy."
  },
  {
    "objectID": "projects/Template/index.html#appendix",
    "href": "projects/Template/index.html#appendix",
    "title": "Example Paper",
    "section": "Appendix",
    "text": "Appendix\nThe entire website, including this example project is located at https://github.com/joshuacharleshyatt/personal"
  },
  {
    "objectID": "qualifications.html#skills",
    "href": "qualifications.html#skills",
    "title": "Joshua Hyatt",
    "section": "Skills",
    "text": "Skills\n\n\n\n\n\n%%{init: {'theme':'neutral'}}%%\nmindmap\n  root((Skills))\n    Machine Learning and AI\n      Supervised & Unsupervised Leaening\n      Anomaly Detection\n      Natural Language Processing\n      Computer Vision\n      Optimization & Simulation\n    Programming and Frameworks\n      Python\n      Bash\n      SQL\n      Spark\n      MLOps\n    Data & BI Tools\n      DOMO\n      Tableau\n      Data Modeling\n      Feature Engineering\n    Platforms and Infrastructure\n      Databricks\n      Gitlab CI/CD\n      Azure Cloud\n      Azure Devops\n      Linux\n    Collaberation and Leadership\n      Agile and Scrum\n      Technical Mentorship\n      Cross Functional Communication/Leadership\n\n\n\n\n\n\n\n\n\n\n\n\nDetailed Skills\n\n\n\n\n\n\nMachine Learning & AI\n\nSupervised & Unsupervised Learning, Time Series Forecasting\nAnomaly Detection (Autoencoders, One-Class SVM, LOF, PCA)\nNLP (Sentence & Word Embeddings, NER, Sentiment Analysis)\nComputer Vision (CNNs, U-Net, Image Embeddings)\nOptimization & Simulation (Gurobi, SEIR Modeling)\n\n\n\nProgramming & Frameworks\n\nLanguages: Python (NumPy, Pandas, SciPy, matplotlib, NLTK, SQLAlchemy, Scikit-learn, TensorFlow, Beautiful Soup, UMAP, XGBoost), SQL, Bash, Spark\nMLOps & DevOps: Git, CI/CD, Docker, MLflow, Pytest, Selenium\n\n\n\nData & BI Tools\n\nSQL (PostgreSQL, MSSQL, Cassandra)\nDOMO, Tableau, Jupyter Notebooks, Quarto\nData Modeling, Feature Engineering, Data Wrangling\n\n\n\nPlatforms & Infrastructure\n\nDatabricks: Jobs, Pipelines, Workspace Administration, Repos, Unity Catalog, Vector Stores\nGitLab (Runner & Pipelines)\nAzure Cloud, Docker (Containerization & Deployment)\nAzure Devops, GitLab Devops, Git Pages\nLinux (Command Line, Scripting), REST API Deployment and Web Hosting\n\n\n\nCollaboration & Leadership\n\nAgile & Scrum Environments\nTechnical Mentorship & Training Programs (Self Led and Instructor Led)\nCross-Functional Communication (Business, Engineering, Executives)\nDocumentation & Reproducibility Practices"
  }
]