[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Example Paper\n\n\n\nPaper\n\n\nPython\n\n\nDataviz\n\n\n\n\nMar 8, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "qualifications.html",
    "href": "qualifications.html",
    "title": "Joshua Hyatt",
    "section": "",
    "text": "I have over five years of experience working as a Data Scientist and many more years developing technical solutions for multinational companies. I have worked in the finance, automotive, and education industries applying my expertise in modeling and software development. My current areas of focus include creating useful embeddings for various applications such as AI Vision for industrial manufacturing as well as NLP for purchasing similarity recommendations."
  },
  {
    "objectID": "qualifications.html#professional-experience",
    "href": "qualifications.html#professional-experience",
    "title": "Joshua Hyatt",
    "section": "Professional Experience",
    "text": "Professional Experience\n\n\n\n\n\n%%{init: {'theme':'neutral'}}%%\ngantt\n  dateFormat YYYY-MM\n  axisformat %y\n  section Education\n    Bachelor          :done, e1, 2007-08, 4y\n    Masters           :done, e2, after e1, 2y\n  section Work Experience\n    Pellissippi State :done, w1, after e2, 2y\n    Elavon            :done, w2, after w1, 5y\n    DENSO             :active, w3, after w2, 6y\n\n\n\n\n\n\n\nData Scientist III\n\n\n\n\n\n\nDENSO International America, INC. Maryville, TN July 2018 – Present\n\n\n\nLeverage analytical and programming skills to extract insights from complex datasets to inform strategic decision-making. Utilize statistical techniques, machine learning algorithms, and data visualization tools to interpret data and solve business problems.Responsibilities include: data cleaning, modeling, deployment, and presenting findings to executive stakeholders.\n\nCollaborated with MIT to map and analyze a significant portion of our supply chain network including conducting simulations (using Gurobi) and risk assessments based on GIS data to mitigate potential disruptions from events like earthquakes and floods.\nSpearheaded development of segmentation-based neural network defect detection algorithms and web UI integration using Flask for multiple inspection robots, increasing quality through improved detection and reducing the number of inspectors.\nImplemented asset monitoring system to leverage anomaly detection (auto-encoders) and forecasting techniques resulting in early detection of potential breakdowns that contribute to hundreds of thousands of dollars in savings.\nOversaw and guided Data Science Internships focused on Natural Language Processing (sentence embedding, UMAP, and NER), enhancing maintenance reporting, and integrating corporate purchases analysis to optimize procurement processes.\nProactively forecasted the peak of COVID cases (SEIR) 16 months in advance with a margin of error of only 8 days, which enabled management to anticipate labor shortages and plan accordingly for the height of the pandemic.\nEstablished a robust Dev/Sec/Ops pipeline in GitLab, which automated testing and deployment of containerized machine learning applications with web APIs for improved efficiency and reliability.\nDevised data collection strategies and developed solution roadmaps for 2 out of 3 key pillars for manufacturing: Predictive Maintenance and AI Vision-based inspection to meet the goals of operational excellence and quality assurance initiatives.\n\n\n\n\n\nBusiness Analyst II ← DBA ← Fraud Analyst\n\n\n\n\n\n\nElavon Knoxville, TN Nov 2015 – July 2018\n\n\n\nAppropriately assigned high-risk fraud accounts to work queues, managed back-end relational databases, and provided analytical reporting needs to management.\n\nIncreased the efficiency of managing work queues by accurately identifying fraud, resulting in $200,000+ additional dollars saved compared to the prior 2-year average.\nDeveloped an application utilizing web scraping that improved the efficiency of identifying and notating potentially risky accounts by 50%. Estimated savings of $25,000-$50,000 annually.\nDirected the Dedicated Solutions Team, an initiative to redesign and simplify technological use within the business through automation and analytics.\nAwarded the Pinnacle Award for excellence in performance company wide (top 10% of employee performance), April 2016.\n\n\n\n\n\nAdjunct Mathematics Professor\n\n\n\n\n\n\nPellissippi State Community College Knoxville, TN Aug 2013 – Nov 2015\n\n\n\nPlanned and held lectures, led in-class discussions, and graded assignments for Statistics, Algebra, and Business Calculus courses."
  },
  {
    "objectID": "qualifications.html#education",
    "href": "qualifications.html#education",
    "title": "Joshua Hyatt",
    "section": "Education",
    "text": "Education\n\n\n\n\n\n\nMasters of Science in Mathematics\nUniversity of Tennessee - Knoxville, TN\nGPA: 3.97/4.0\n\n\n\n\nBachelor of Science in Mathematics\nMurray State University - Murray, KY\nGPA: 3.81/4.0"
  },
  {
    "objectID": "qualifications.html#expertise",
    "href": "qualifications.html#expertise",
    "title": "Joshua Hyatt",
    "section": "Expertise",
    "text": "Expertise\n\n\n\n\n\n%%{init: {'theme':'neutral'}}%%\nmindmap\n  root((Skills))\n    Analytics\n      Natural Language Processing\n      Anomaly Detection\n      Convolutional Neural Networks\n      Business Intelligence\n      Literate Progamming\n    Programming\n      Python\n      GIT\n      SQL\n      Linux\n    Deployment\n      Docker\n      CI/CD\n      Automated Testing\n\n\n\n\n\n\n\n\n\n\n\n\nDetailed Descriptions\n\n\n\n\n\n\nAnalytics\n\nNatural Language Processing (Word and Sentence embedding, Sentiment Analysis)\nAnomaly Detection (Auto-Encoders, One Class SVM, Local Outlier Factor, PCA)\nConvolutional Neural Networks (Auto-Encoders, U-net, Image Embedding)\nBI (Tableau, DOMO)\nLiterate Programming (Jupyter Notebooks, Quarto, Codebraid)\n\n\n\nProgramming\n\nPython (NumPy, Pandas, gurobipy, GeoPandas, Scikit-learn, Tensorflow, SciPy, Flask, NLTK, SQLAlchemy, NetworkX, matplotlib, BaseMap, shapely, Beautiful Soup)\nGIT\nSQL (Postgres, MSSQL, Cassandra, etc)\nLinux (including terminal)\n\n\n\nDeployment\n\nDocker Containerization\nCI/CD Pipeline (Gitlab Runner)\nAutomated Testing (Pytest, Selenium Web Testing)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joshua Hyatt",
    "section": "",
    "text": "I have more than 5 years of experience working as a Data Scientist and many more years developing technical solutions for multinational companies. I have worked in the Finance, Automotive, and Education industries, applying my expertise in modeling and software development. At heart, I am a math nerd, receiving my master’s in Mathematics at the University of Tennessee. My current areas of focus include creating useful embeddings for various applications such as AI Vision for industrial manufacturing as well as NLP for purchasing similarity recommendations."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Joshua Hyatt",
    "section": "",
    "text": "I have more than 5 years of experience working as a Data Scientist and many more years developing technical solutions for multinational companies. I have worked in the Finance, Automotive, and Education industries, applying my expertise in modeling and software development. At heart, I am a math nerd, receiving my master’s in Mathematics at the University of Tennessee. My current areas of focus include creating useful embeddings for various applications such as AI Vision for industrial manufacturing as well as NLP for purchasing similarity recommendations."
  },
  {
    "objectID": "projects/Template/index.html",
    "href": "projects/Template/index.html",
    "title": "Example Paper",
    "section": "",
    "text": "Code\nimport io\nfrom sklearn import get_config\nfrom config import save_system\nfrom encryption import RandomState\nimport tomli\nfrom contextlib import redirect_stdout\n\n# remove stupid advert\nf = io.StringIO()\nwith redirect_stdout(f):\n  import ydata_profiling as yd\n\nfrom pycaret.datasets import get_data\nfrom pycaret.classification.functional import setup, get_config, compare_models, create_model, pull, finalize_model, save_model, plot_model\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams['font.family'] = 'DeJavu Serif'\nplt.rcParams['font.serif'] = ['Times New Roman']\nfrom yellowbrick.features import rank2d\nimport yellowbrick as yb\nimport dtreeviz\nCode\nsave_system()\n\nwith open(\"../../pyproject.toml\", mode=\"rb\") as fp:\n  config = tomli.load(fp)\n\nPROJECT_NAME = config[\"tool\"][\"poetry\"][\"name\"]\nRANDOM_STATE = RandomState(PROJECT_NAME).state"
  },
  {
    "objectID": "projects/Template/index.html#introduction",
    "href": "projects/Template/index.html#introduction",
    "title": "Example Paper",
    "section": "Introduction",
    "text": "Introduction\nBioinformatics is an emerging and rapidly evolving field focused on extracting meaningful insights from biological data using computational techniques. Fundamental challenges in bioinformatics, such as protein structure prediction, sequence alignment, and phylogenetic inference, are often computationally complex and classified as NP-hard problems. Machine learning (ML) methods, including Artificial Neural Networks (ANN), Fuzzy Logic, Genetic Algorithms, and Support Vector Machines (SVM), have shown promise in addressing these challenges.\nThis study focuses on classifying Iris species using measurements of sepal length, sepal width, petal length, and petal width. The Iris dataset, introduced by R.A. Fisher in 1936, has become a standard benchmark for classification models. The goal is to evaluate the effectiveness of SVM for classification and explore the impact of dimensionality reduction using PCA on model performance. Decision Tree models are also assessed for comparison, balancing accuracy with interpretability."
  },
  {
    "objectID": "projects/Template/index.html#data",
    "href": "projects/Template/index.html#data",
    "title": "Example Paper",
    "section": "Data",
    "text": "Data\nThe Iris dataset, obtained from the UCI Machine Learning Repository, consists of 150 instances representing three species of Iris plants:\n\nIris Setosa\nIris Versicolor\nIris Virginica\n\nEach species is represented by 50 samples. The dataset includes four numeric attributes:\n\nSepal length (cm)\nSepal width (cm)\nPetal length (cm)\nPetal width (cm)\n\n\n\nCode\ndata = get_data(\"iris\")\ndecoder = ['setosa', 'versicolor', 'virginica']\n\n\n\n\n\n\nIris Measures\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\nThe fifth attribute is the class label indicating the species. The dataset is complete, with no missing values or inconsistencies reported. While the Setosa species is separable from the other two species, Versicolor and Virginica exhibit significant overlap, making classification the challenge of the study. A copy of the exploratory data report can be found here EDA Report\n\n\nCode\nreport = data.profile_report(progress_bar=False, title=\"Iris EDA Report\")\nreport.to_file(\"../../assets/Iris_EDA.html\", silent = True)"
  },
  {
    "objectID": "projects/Template/index.html#methods",
    "href": "projects/Template/index.html#methods",
    "title": "Example Paper",
    "section": "Methods",
    "text": "Methods\nPrincipal Component Analysis (PCA) is a widely used dimensionality reduction technique that can project data onto a lower-dimensional subspace while retaining the maximum variance. PCA helps reduce the computational complexity of the classification task, but more importantly in this instance, improves visualization.\nIn this study, PCA reduces the four-dimensional Iris data into two principal components:\n\nFirst Principal Component: Accounts for the highest variance in the data.\nSecond Principal Component: Captures the next highest variance.\n\n\n\nCode\nenv_pca = setup(data, target=\"species\", train_size = .8, session_id = RANDOM_STATE, pca=True, pca_components = 2, verbose = False, normalize = True)\n\n\n\n\nCode\nenv_pca.X_transformed.head()\n\n\n\n\n\n\nReduced Dimension Measurements\n\n\n\npca0\npca1\n\n\n\n\n83\n-1.069254\n-0.680773\n\n\n19\n2.308932\n1.167804\n\n\n84\n-0.230247\n-0.329121\n\n\n52\n-1.329566\n0.578077\n\n\n81\n0.044678\n-1.574244\n\n\n\n\n\n\n\nBy plotting the data along these two principal components, a clear separation between the Setosa species and the other two species becomes visible. However, Versicolor and Virginica still exhibit considerable overlap.\n\n\nCode\npca = env_pca.pipeline.named_steps[\"pca\"].transformer\nplt.figure()\nsns.scatterplot(env_pca.X_transformed, x=\"pca0\", y=\"pca1\", hue=data[\"species\"]).set(title=\"IRIS Species Scatter Plot \\n Explained Variance: %.3f\" % pca.explained_variance_ratio_.sum())\n\n\n\n\n\n\n\n\n\n\nFeature Importance\nWhile PCA is great for dimensionality reduction and visualization, we do lose some interpretability in how the scientific measurements are correlated to the target species. For this, we look at feature rank and feature importance charts to determine the measurements most affecting the separation between classes.\n\n\nCode\nenv_norm = setup(data, target=\"species\", train_size = .8, session_id = RANDOM_STATE, verbose = False, normalize=True)\n\n\n\nCode\nfig = rank2d(get_config(\"X_train\"), get_config(\"y_train\"))\nfig = yb.target.feature_correlation.feature_correlation(env_norm.X_train, env_norm.y_train, method='mutual_info-classification')\n\n\n\n\n\n\n\nPairwise Feature Correlation\n\n\n\n\n\n\n\nTarget Correlation\n\n\n\n\n\n Standard practice is to perform train test split on the transformed data to evaluate the model performance. We will use the convention that splits the data in 80% training/validation and 20% test set. Because the sample size is relatively small, we will use cross validation to prevent overfitting. The advantages of this technique are the ability to limit overfitting on a relatively small dataset.\n\n\nCode\nfig = yb.target.class_balance(env_norm.y_train, env_norm.y_test, labels = decoder)"
  },
  {
    "objectID": "projects/Template/index.html#analysis",
    "href": "projects/Template/index.html#analysis",
    "title": "Example Paper",
    "section": "Analysis",
    "text": "Analysis\nThe models were evaluated using an 80/20 train-test split with five-fold cross-validation to mitigate overfitting. Evaluation metrics included:\n\nPrecision – The proportion of correct positive predictions.\nRecall – The proportion of actual positives correctly identified.\nF1 Score – The harmonic mean of precision and recall, providing a balanced measure of model accuracy.\nConfusion Matrix - Illustrates misclassification patterns across species.\n\n\nClassification by Support Vector Machine\nSupport Vector Machines (SVM) are effective for high-dimensional classification problems. SVM constructs a hyperplane in a multi-dimensional space that maximally separates the classes. In cases where the data is not linearly separable, SVM employs a kernel trick to map the data into a higher-dimensional space where linear separation becomes possible.\n\n\nCode\nsvm_model = create_model(\"svm\", tol=1e-3, alpha=.0012, verbose=False) # parameters can be found with tune_model\npull()\n\n\n\n\n\n\nCross Validation Results\n\n\n\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n\n\n\n\n\n\n\n\n\n\n\n0\n0.7500\n0.0\n0.7500\n0.7556\n0.7460\n0.6250\n0.6316\n\n\n1\n0.9167\n0.0\n0.9167\n0.9333\n0.9153\n0.8750\n0.8843\n\n\n2\n0.9167\n0.0\n0.9167\n0.9333\n0.9153\n0.8750\n0.8843\n\n\n3\n0.9167\n0.0\n0.9167\n0.9333\n0.9153\n0.8750\n0.8843\n\n\n4\n1.0000\n0.0\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n5\n0.8333\n0.0\n0.8333\n0.8333\n0.8333\n0.7500\n0.7500\n\n\n6\n1.0000\n0.0\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n7\n1.0000\n0.0\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n8\n0.9167\n0.0\n0.9167\n0.9333\n0.9153\n0.8750\n0.8843\n\n\n9\n0.8333\n0.0\n0.8333\n0.8889\n0.8222\n0.7500\n0.7833\n\n\nMean\n0.9083\n0.0\n0.9083\n0.9211\n0.9063\n0.8625\n0.8702\n\n\nStd\n0.0786\n0.0\n0.0786\n0.0744\n0.0805\n0.1179\n0.1141\n\n\n\n\n\n\n\n\n\nComparison to Decision Tree\nA Decision Tree classifier was used as a benchmark to compare with the SVM model. Decision Trees provide an interpretable model by recursively splitting the data based on the most informative features. The tree structure allows for straightforward interpretation of how classifications are made.\n\n\nCode\ntree_model = create_model(\"dt\", max_depth=3, verbose = False)\nwinning_model = compare_models(include=[svm_model, tree_model], verbose = False)\npull()\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\nTT (Sec)\n\n\n\n\n1\nDecision Tree Classifier\n0.9500\n0.9677\n0.9500\n0.9600\n0.9492\n0.9250\n0.9306\n0.005\n\n\n0\nSVM - Linear Kernel\n0.9083\n0.0000\n0.9083\n0.9211\n0.9063\n0.8625\n0.8702\n0.008\n\n\n\n\n\n\n\n\nCode\nfig = plot_model(svm_model, plot = \"confusion_matrix\", plot_kwargs = {\"classes\": decoder})\nfig = plot_model(tree_model, plot = \"confusion_matrix\", plot_kwargs = {\"classes\": decoder})\nfig = plot_model(svm_model, plot = \"class_report\", plot_kwargs = {\"classes\": decoder})\nfig = plot_model(tree_model, plot = \"class_report\", plot_kwargs = {\"classes\": decoder})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\nThe Decision Tree model achieved comparable performance, almost identical to the Support Vector Machine. This is a common phenomenon in machine learning applications and often greatly ignored by auto=ml packages. Decision Trees, provide a more transparent decision-making process, which is valuable for field applications where understanding the classification process is essential.\n\n\nCode\nmessage = save_model(svm_model, \"models/svm-model\", verbose = False)\nmessage = save_model(tree_model, \"models/tree-model\", verbose = False)\n\n\n\n\nModel Interpretability\nA simple decision tree can be printed out as a flowchart or a series of branching yes/no questions, making it easy to use in field work where quick decisions are necessary. Each node in the tree represents a question based on a specific feature, such as “Is petal length greater than 2.5 cm?” The branches lead to subsequent questions or to a classification decision, such as identifying the plant as Setosa or Versicolor. This format allows field researchers to visually trace the decision path step-by-step, even without access to computational tools. The straightforward structure of a decision tree makes it easy to understand and follow, enabling non-experts to accurately classify samples based on observable characteristics. The transparency and simplicity of the printed decision tree make it particularly valuable for practical applications in biological fieldwork.\n\n\nCode\n# Build a model on the final data for deployment.\nenv_prod = setup(data, target=\"species\", train_size = .8, session_id = RANDOM_STATE, verbose = False) # trees don't need normalization\nfinal_tree = finalize_model(tree_model)\nmessage = save_model(final_tree, \"models/production-model\", verbose = False)\n\n\n\n\nCode\n# Visualize predictions\nsample = env_prod.X_transformed.iloc[120] \nfinal_tree_model = final_tree.named_steps['actual_estimator']\nviz_model =  dtreeviz.model(final_tree_model, X_train=env_prod.X_transformed, y_train = env_prod.y_transformed, feature_names = list(env_prod.X_transformed), target_name = \"species name\", class_names = decoder)\nviz_model.view(x = sample, fontname=\"DejaVu Sans\")\n\n\n\n\n\n\n\n\n\nIn-case displaying the entire tree is not desirable especially in regulatory environments, we can instead display the list of features and their importance related to the decision.\n\n\nCode\nviz_model.instance_feature_importance(sample, fontname=\"DejaVu Sans\")"
  },
  {
    "objectID": "projects/Template/index.html#conclusion",
    "href": "projects/Template/index.html#conclusion",
    "title": "Example Paper",
    "section": "Conclusion",
    "text": "Conclusion\n\nDiscussion\nMisclassification between Versicolor and Virginica reflects the biological similarities in their sepal and petal measurements. Future research could explore additional botanical features or incorporate other ML techniques, such as ensemble methods or deep learning, to improve discrimination between these species.\nSVM remains a powerful classification tool for complex, high-dimensional datasets. However, the increased interpretability of Decision Trees suggests that they may be more suitable for practical, real-world applications in biological research.\n\n\nConclusion\nThis study demonstrates the effectiveness of SVM in classifying Iris species based on sepal and petal measurements. PCA successfully reduced the data dimensionality, improving visualization. SVM achieves high accuracy, however, the interpretability of Decision Trees makes them a valuable alternative for practical applications where model transparency is critical.\n\n\nFuture Work\nFuture work could explore:\n\nIncorporating additional morphological features to improve classification between Versicolor and Virginica.\nTesting ensemble methods such as Random Forest or Gradient Boosted Trees for enhanced accuracy."
  },
  {
    "objectID": "projects/Template/index.html#appendix",
    "href": "projects/Template/index.html#appendix",
    "title": "Example Paper",
    "section": "Appendix",
    "text": "Appendix\nThe entire website, including this example project is located at https://github.com/joshuacharleshyatt/personal"
  }
]