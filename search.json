[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Example Paper\n\n\n\nPaper\n\n\nPython\n\n\nDataviz\n\n\n\n\nMar 8, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "qualifications.html",
    "href": "qualifications.html",
    "title": "Joshua Hyatt",
    "section": "",
    "text": "I have over five years of experience working as a Data Scientist and many more years developing technical solutions for multinational companies. I have worked in the finance, automotive, and education industries applying my expertise in modeling and software development. My current areas of focus include creating useful embeddings for various applications such as AI Vision for industrial manufacturing as well as NLP for purchasing similarity recommendations."
  },
  {
    "objectID": "qualifications.html#professional-experience",
    "href": "qualifications.html#professional-experience",
    "title": "Joshua Hyatt",
    "section": "Professional Experience",
    "text": "Professional Experience\n\n\n\n\n\n%%{init: {'theme':'neutral'}}%%\ngantt\n  dateFormat YYYY-MM\n  axisformat %y\n  section Education\n    Bachelor          :done, e1, 2007-08, 4y\n    Masters           :done, e2, after e1, 2y\n  section Work Experience\n    Pellissippi State :done, w1, after e2, 2y\n    Elavon            :done, w2, after w1, 5y\n    DENSO             :active, w3, after w2, 6y\n\n\n\n\n\n\n\nData Scientist III\n\n\n\n\n\n\nDenso International America, INC. Maryville, TN July 2018 – Present\n\n\n\nLeverage analytical and programming skills to extract insights from complex datasets to inform strategic decision-making. Utilize statistical techniques, machine learning algorithms, and data visualization tools to interpret data and solve business problems.Responsibilities include: data cleaning, modeling, deployment, and presenting findings to executive stakeholders.\n\nCollaborated with MIT to map and analyze a significant portion of our supply chain network including conducting simulations (using Gurobi) and risk assessments based on GIS data to mitigate potential disruptions from events like earthquakes and floods.\nSpearheaded development of segmentation-based neural network defect detection algorithms and web UI integration using Flask for multiple inspection robots, increasing quality through improved detection and reducing the number of inspectors.\nImplemented asset monitoring system to leverage anomaly detection (auto-encoders) and forecasting techniques resulting in early detection of potential breakdowns that contribute to hundreds of thousands of dollars in savings.\nOversaw and guided Data Science Internships focused on Natural Language Processing (sentence embedding, UMAP, and NER), enhancing maintenance reporting, and integrating corporate purchases analysis to optimize procurement processes.\nProactively forecasted the peak of COVID cases (SEIR) 16 months in advance with a margin of error of only 8 days, which enabled management to anticipate labor shortages and plan accordingly for the height of the pandemic.\nEstablished a robust Dev/Sec/Ops pipeline in GitLab, which automated testing and deployment of containerized machine learning applications with web APIs for improved efficiency and reliability.\nDevised data collection strategies and developed solution roadmaps for 2 out of 3 key pillars for manufacturing: Predictive Maintenance and AI Vision-based inspection to meet the goals of operational excellence and quality assurance initiatives.\n\n\n\n\n\nBusiness Analyst II ← DBA ← Fraud Analyst\n\n\n\n\n\n\nElavon Knoxville, TN Nov 2015 – July 2018\n\n\n\nAppropriately assigned high-risk fraud accounts to work queues, managed backend relational databases, and provided analytical reporting needs to management.\n\nIncreased the efficiency of managing work queues by accurately identifying fraud, resulting in $200,000+ additional dollars saved compared to the prior 2-year average.\nDeveloped an application utilizing web scraping that improved the efficiency of identifying and notating potentially risky accounts by 50%. Estimated savings of $25,000-$50,000 annually.\nDirected the Dedicated Solutions Team, an initiative to redesign and simplify technological use within the business through automation and analytics.\nAwarded the Pinnacle Award for excellence in performance company wide (top 10% of employee performance), April 2016.\n\n\n\n\n\nAdjunct Mathematics Professor\n\n\n\n\n\n\nPellissippi State Community College Knoxville, TN Aug 2013 – Nov 2015\n\n\n\nPlanned and held lectures, led in-class discussions, and graded assignments for Statistics, Algebra, and Business Calculus courses."
  },
  {
    "objectID": "qualifications.html#education",
    "href": "qualifications.html#education",
    "title": "Joshua Hyatt",
    "section": "Education",
    "text": "Education\n\n\n\n\n\n\nMasters of Science in Mathematics\nUniversity of Tennessee - Knoxville, TN\nGPA: 3.97/4.0\n\n\n\n\nBachelor of Science in Mathematics\nMurray State University - Murray, KY\nGPA: 3.81/4.0"
  },
  {
    "objectID": "qualifications.html#expertise",
    "href": "qualifications.html#expertise",
    "title": "Joshua Hyatt",
    "section": "Expertise",
    "text": "Expertise\n\n\n\n\n\n%%{init: {'theme':'neutral'}}%%\nmindmap\n  root((Skills))\n    Analytics\n      Natural Language Processing\n      Anomaly Detection\n      Convolutional Neural Networks\n      Business Intelligence\n      Literate Progamming\n    Programming\n      Python\n      GIT\n      SQL\n      Linux\n    Deployment\n      Docker\n      CI/CD\n      Automated Testing\n\n\n\n\n\n\n\n\n\n\n\n\nDetailed Descriptions\n\n\n\n\n\n\nAnalytics\n\nNatural Language Processing (Word and Sentence embedding, Sentiment Analysis)\nAnomaly Detection (Auto-Encoders, One Class SVM, Local Outlier Factor, PCA)\nConvolutional Neural Networks (Auto-Encoders, U-net, Image Imbedding)\nBI (Tableau, DOMO)\nLiterate Programming (Jupyter Notebooks, Quarto, Codebraid)\n\n\n\nProgramming\n\nPython (NumPy, Pandas, gurobipy, GeoPandas, Scikit-learn, Tensorflow, SciPy, Flask, NLTK, SQLAlchemy, NetworkX, matplotlib, BaseMap, shapely, Beautiful Soup)\nGIT\nSQL (Postgres, MSSQL, Cassandra, etc)\nLinux (including terminal)\n\n\n\nDeployment\n\nDocker Containerization\nCI/CD Pipeline (Gitlab Runner)\nAutomated Testing (Pytest, Selenium Web Testing)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Joshua Hyatt",
    "section": "",
    "text": "I have more than 5 years of experience working as a Data Scientist and many more years developing technical solutions for multinational companies. I have worked in the Finance, Automotive, and Education industries, applying my expertise in modeling and software development. At heart, I am a math nerd, receiving my master’s in Mathematics at the University of Tennessee. My current areas of focus include creating useful embeddings for various applications such as AI Vision for industrial manufacturing as well as NLP for purchasing similarity recommendations."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Joshua Hyatt",
    "section": "",
    "text": "I have more than 5 years of experience working as a Data Scientist and many more years developing technical solutions for multinational companies. I have worked in the Finance, Automotive, and Education industries, applying my expertise in modeling and software development. At heart, I am a math nerd, receiving my master’s in Mathematics at the University of Tennessee. My current areas of focus include creating useful embeddings for various applications such as AI Vision for industrial manufacturing as well as NLP for purchasing similarity recommendations."
  },
  {
    "objectID": "projects/Template/index.html",
    "href": "projects/Template/index.html",
    "title": "Example Paper",
    "section": "",
    "text": "Code\nimport io\nfrom sklearn import get_config\nfrom config import save_system\nfrom encryption import RandomState\nimport tomli\nfrom contextlib import redirect_stdout\n\n# remove stupid advert\nf = io.StringIO()\nwith redirect_stdout(f):\n  import ydata_profiling as yd\n\nfrom pycaret.datasets import get_data\nfrom pycaret.classification.functional import setup, get_config, compare_models, create_model, pull, finalize_model, save_model, plot_model, evaluate_model\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams['font.family'] = 'DeJavu Serif'\nplt.rcParams['font.serif'] = ['Times New Roman']\nfrom yellowbrick.features import rank2d\nimport yellowbrick as yb\nimport dtreeviz\nCode\nsave_system()\n\nwith open(\"../../pyproject.toml\", mode=\"rb\") as fp:\n  config = tomli.load(fp)\n\nPROJECT_NAME = config[\"tool\"][\"poetry\"][\"name\"]\nRANDOM_STATE = RandomState(PROJECT_NAME).state"
  },
  {
    "objectID": "projects/Template/index.html#introduction",
    "href": "projects/Template/index.html#introduction",
    "title": "Example Paper",
    "section": "Introduction",
    "text": "Introduction\nBio-informatics is a promising and novel research area in the 21st century. This field is data driven and aims at understanding of relationships and gaining knowledge in biology. In order to extract this knowledge encoded in biological data, advanced computational technologies, algorithms and tools need to be used. Basic problems in bio-informatics like protein structure prediction, multiple alignments of sequences, phylogenic inferences, etc are inherently nondeterministic polynomial-time hard in nature. To solve these kinds of problems artificial intelligence (AI) methods offer a powerful and efficient approach. Researchers have used AI techniques like Artificial Neural Networks (ANN), Fuzzy Logic, Genetic Algorithms, and Support Vector Machines to solve problems in bio-informatics.\nThe concern of this study is towards the identification of IRIS plants on the basis of the following measurements: sepal length, sepal width, petal length, and petal width. This paper is mainly concerned with an analysis of the performance results of classification techniques such as Decision Tree, Support Vector Machine (SVM) in determining the species of the Iris plants."
  },
  {
    "objectID": "projects/Template/index.html#data",
    "href": "projects/Template/index.html#data",
    "title": "Example Paper",
    "section": "Data",
    "text": "Data\nOne of the most popular and best known databases of the neural network application is the IRIS plant data set which is obtained from UCI Machine Learning Repository and created by R.A. Fisher while donated by Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) on July, 1988\nThe IRIS dataset classifies three different classes of IRIS plant by performing pattern classification. The IRIS data set includes three classes of 50 objects each, where each class refers to a type of IRIS plant. The attributed that already been predicted belongs to the class of IRIS plant. The list of attributes present in the IRIS can be described as categorical, nominal and continuous. The experts have mentioned that there isn’t any missing value found in any attribute of this data set. The data set is complete.\nThis project makes use of the well known IRIS dataset, which refers to three classes of 50 instances each, where each class refers to a type of IRIS plant. The first of the classes is linearly distinguishable from the remaining two, with the second two not being linearly separable from each other. The 150 instances, which are equally separated between the three classes, contain the following four numeric attributes:\n\n\nCode\ndata = get_data(\"iris\")\ndecoder = ['setosa', 'versicolor', 'virginica']\n\n\n\n\n\n\nIris Measures\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nIris-setosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nIris-setosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nIris-setosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nIris-setosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nIris-setosa\n\n\n\n\n\n\n\nThe fifth attribute is the predictive attributes which is the class attribute that means each instance also includes an identifying class name, each of which is one of the following: IRIS Setosa, IRIS Versicolour, or IRIS Virginica. A copy of the report can be found here EDA Report\n\n\nCode\nreport = data.profile_report(progress_bar=False, title=\"Iris EDA Report\")\nreport.to_file(\"../../assets/Iris_EDA.html\", silent = True)"
  },
  {
    "objectID": "projects/Template/index.html#methods",
    "href": "projects/Template/index.html#methods",
    "title": "Example Paper",
    "section": "Methods",
    "text": "Methods\nDimensionality reduction is a really important concept in Machine Learning since it reduces the number of features in a dataset and hence reduces the computations needed to fit the model. PCA is one of the well known efficient dimensionality reduction techniques. in this tutorial we will use PCA which compresses the data by projecting it to a new subspace that can help in reducing the effect of the curse of dimensionality. Our dataset consists of 4 dimensions(4 features) so we will project it to a 2 dimensions space and plot it for visualization.\n\n\nCode\nenv_pca = setup(data, target=\"species\", train_size = .8, session_id = RANDOM_STATE, pca=True, pca_components = 2, verbose = False, normalize = True)\n\n\n\n\nCode\nenv_pca.X_transformed.head()\n\n\n\n\n\n\nReduced Dimension Measurements\n\n\n\npca0\npca1\n\n\n\n\n83\n-1.069254\n-0.680773\n\n\n19\n2.308932\n1.167804\n\n\n84\n-0.230247\n-0.329121\n\n\n52\n-1.329566\n0.578077\n\n\n81\n0.044678\n-1.574244\n\n\n\n\n\n\n\nThe first principle component has the highest explained variance, followed by the 2nd component. By plotting these 2 components, we get a view of the 4 dimensions with as much explained variance as possible for a linear transformation. Although not a guarantee to be beneficial, often as in this case, clear separation of the target class can be observed in the transformed subspace.\n\n\nCode\npca = env_pca.pipeline.named_steps[\"pca\"].transformer\nplt.figure()\nsns.scatterplot(env_pca.X_transformed, x=\"pca0\", y=\"pca1\", hue=data[\"species\"]).set(title=\"IRIS Species Scatter Plot \\n Explained Variance: %.3f\" % pca.explained_variance_ratio_.sum())\n\n\n\n\n\n\n\n\n\n\nFeature Importance\nWhile PCA is great for dimensionality reduction and visualization, we do lose some interpretability in how the scientific measurements are correlated to the target species. For this, we look at feature rank and feature importance charts to determine the measurements most affecting the separation between classes.\n\n\nCode\nenv_norm = setup(data, target=\"species\", train_size = .8, session_id = RANDOM_STATE, verbose = False, normalize=True)\n\n\n\nCode\nfig = rank2d(get_config(\"X_train\"), get_config(\"y_train\"))\nfig = yb.target.feature_correlation.feature_correlation(env_norm.X_train, env_norm.y_train, method='mutual_info-classification')\n\n\n\n\n\n\n\nPairwise Feature Correlation\n\n\n\n\n\n\n\nTarget Correlation\n\n\n\n\n\n Standard practice is to perform train test split on the transformed data to evaluate the model performance. We will use the convention that splits the data in 80% training/validation and 20% test set. Because the sample size is relatively small, we will use cross validation to prevent overfitting. We set the random state to a constant value in order to get consistent results when we rerun the code. The advantages of this technique are the ability to limit overfitting on a relatively small dataset.\n\n\nCode\nfig = yb.target.class_balance(env_norm.y_train, env_norm.y_test, labels = decoder)"
  },
  {
    "objectID": "projects/Template/index.html#analysis",
    "href": "projects/Template/index.html#analysis",
    "title": "Example Paper",
    "section": "Analysis",
    "text": "Analysis\nIn this section, we analyze how our primary models performed, and discuss reasons why they did not perform even better. Specifically, we will analyze why certain species were commonly confused with others. Confusing versicolor and virginica is understandable based on the boundary overlap in the PCA analysis.\nThe results of the SVM model on unseen data are measured on the f1 score, a geometric mean of the precision and recall scores. This measure is considered a more realistic measure of the overall accuracy of the classification model. Of interest is the variation in performance on each cross validation run. A standard deviation in the range of 5%-10% may see performance results on test or production data far inferior than what is reported in validation.\n\n\nCode\n# Modeling\nsvm_model = create_model(\"svm\", tol=1e-3, alpha=.0012, verbose=False) # parameters can be found with tune_model\npull()\n\n\n\n\n\n\nCross Validation Results\n\n\n\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n\n\n\n\n\n\n\n\n\n\n\n0\n0.7500\n0.0\n0.7500\n0.7556\n0.7460\n0.6250\n0.6316\n\n\n1\n0.9167\n0.0\n0.9167\n0.9333\n0.9153\n0.8750\n0.8843\n\n\n2\n0.9167\n0.0\n0.9167\n0.9333\n0.9153\n0.8750\n0.8843\n\n\n3\n0.9167\n0.0\n0.9167\n0.9333\n0.9153\n0.8750\n0.8843\n\n\n4\n1.0000\n0.0\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n5\n0.8333\n0.0\n0.8333\n0.8333\n0.8333\n0.7500\n0.7500\n\n\n6\n1.0000\n0.0\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n7\n1.0000\n0.0\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n8\n0.9167\n0.0\n0.9167\n0.9333\n0.9153\n0.8750\n0.8843\n\n\n9\n0.8333\n0.0\n0.8333\n0.8889\n0.8222\n0.7500\n0.7833\n\n\nMean\n0.9083\n0.0\n0.9083\n0.9211\n0.9063\n0.8625\n0.8702\n\n\nStd\n0.0786\n0.0\n0.0786\n0.0744\n0.0805\n0.1179\n0.1141\n\n\n\n\n\n\n\nFor comparison, a simple decision tree was trained as well. The following metrics demonstrate the varying performance on each class of the models. Standard metrics for classification are precision, recall and f1 score for each class as well as a confusion matrix. A ROC curve would also be included to demonstrate how the model performance can very even within a confusion matrix if you take into account a rank function. Such a rank function is generally only available for models as using probability based classification such as logistic regression.\n\n\nCode\ntree_model = create_model(\"dt\", max_depth=3, verbose = False)\nwinning_model = compare_models(include=[svm_model, tree_model], verbose = False)\npull()\n\n\n\n\n\n\n\n\n\nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\nTT (Sec)\n\n\n\n\n1\nDecision Tree Classifier\n0.9500\n0.9677\n0.9500\n0.9600\n0.9492\n0.9250\n0.9306\n0.005\n\n\n0\nSVM - Linear Kernel\n0.9083\n0.0000\n0.9083\n0.9211\n0.9063\n0.8625\n0.8702\n0.008\n\n\n\n\n\n\n\nMost sources would indicate at this point to choose the best performing model and then run a test on the holdout dataset. It is important to make a model selection before running a holdout test as the performance results of each model may vary on the holdout set and the winning model may change. Because the purpose of the holdout set is to evaluate a model on unseen data, we cannot retroactively choose the best model for said test.\nFor demonstration purposes, the performance results for both SVM and Decision Tree on the test set are shown. In the next section, we explain why we may choose a certain model even if inferior in performance.\n\nCode\nfig = plot_model(svm_model, plot = \"confusion_matrix\", plot_kwargs = {\"classes\": decoder})\nfig = plot_model(tree_model, plot = \"confusion_matrix\", plot_kwargs = {\"classes\": decoder})\nfig = plot_model(svm_model, plot = \"class_report\", plot_kwargs = {\"classes\": decoder})\nfig = plot_model(tree_model, plot = \"class_report\", plot_kwargs = {\"classes\": decoder})\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1\n\n\n\n\n\nCode\nmessage = save_model(svm_model, \"models/svm-model\", verbose = False)\nmessage = save_model(tree_model, \"models/tree-model\", verbose = False)\n\n\n\nModel Interpretability\nUnfortunately SVM uses a higher dimensional space to linearly slice the sample space into class specific hyperplanes. Visualizing how this is down is difficult, but understood well geometrically. However, the decision tree can easily be visualized and explain the predictive power of the model on the test set. This example shows a mislabeled iris, but more importantly demonstrates how that incorrect result was obtained.\n\n\nCode\n# Build a model on the final data for deployment.\nenv_prod = setup(data, target=\"species\", train_size = .8, session_id = RANDOM_STATE, verbose = False) # trees don't need normalization\nfinal_tree = finalize_model(tree_model)\nmessage = save_model(final_tree, \"models/production-model\", verbose = False)\n\n\n\n\nCode\n# Visualize predictions\nsample = env_prod.X_transformed.iloc[120] \nfinal_tree_model = final_tree.named_steps['actual_estimator']\nviz_model =  dtreeviz.model(final_tree_model, X_train=env_prod.X_transformed, y_train = env_prod.y_transformed, feature_names = list(env_prod.X_transformed), target_name = \"species name\", class_names = decoder)\nviz_model.view(x = sample, fontname=\"DejaVu Sans\")\n\n\n\n\n\n\n\n\n\nIn-case displaying the entire tree is not desirable especially in regulatory environemts, we can instead display the list of features and their importance related to the decision.\n\n\nCode\nviz_model.instance_feature_importance(sample, fontname=\"DejaVu Sans\")\n#explanation = eli5.explain_prediction(final_tree, env_prod.X.iloc[106], target_names = decoder)\n#print(eli5.format_as_text(explanation), f\"True Class: {decoder[env_prod.y[106]]}\")"
  },
  {
    "objectID": "projects/Template/index.html#conclusion",
    "href": "projects/Template/index.html#conclusion",
    "title": "Example Paper",
    "section": "Conclusion",
    "text": "Conclusion\nWe have constructed a relatively accurate classifier for species of Iris flowers based on the field measurements. Using PCA analysis to visualize the data there were clear segments that could be made to classify the various species. Feeding this data into a Support Vector Machine (SVM), we were able to achieve a relatively high F1 score with cross validation as well as the test set, thus confirming validation results. Similarly great results were shown for the Decision Tree model as well.\nBecause SVM is difficult to perform in the fields, and PCA transforms the data into features that represent eigenvectors, this method will prove difficult to interpret as simple rules of known biological dimensions. For field use, it will be of interest to use other classification methods such as decision tree that is both as accurate if not more soe than SVM and easier to use and understand in the field for biological classification.\nOf further interest is the closeness of the versicolor and virginica species with regard to the field measurements. Future studies may explore finding another measure to help differentiate them more as we see there is some similarities in their measurements. This would greatly increase the performance of any model for the purpose of classification."
  },
  {
    "objectID": "projects/Template/index.html#appendix",
    "href": "projects/Template/index.html#appendix",
    "title": "Example Paper",
    "section": "Appendix",
    "text": "Appendix\nThe entire website, including this example project is located at https://github.com/joshuacharleshyatt/personal"
  }
]